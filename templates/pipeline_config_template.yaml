# UrbanRepML Configuration File
# Supports both single runs and experimental parameter sweeps

# Experiment Configuration
experiment:
  # Experiment mode: "single" or "sweep"
  mode: "single"  # Change to "sweep" for parameter exploration
  
  # Experiment metadata
  name: "baseline_south_holland"
  description: "Baseline configuration for South Holland urban embeddings"
  version: "1.0"
  
  # For sweep mode: define parameter ranges
  sweep_config:
    # Sweep strategy: "grid", "random", "bayesian"
    strategy: "grid"
    
    # Parameters to sweep (uncomment to activate)
    parameters:
      # model.conv_type: ["gcn", "gat", "sage"]
      # model.params.hidden_dim: [64, 128, 256]
      # model.params.output_dim: [16, 32, 64]
      # training.optimizer.type: ["adam", "adamw", "sgd"]
      # training.optimizer.params.lr: [1e-6, 1e-5, 1e-4]
      # training.losses.reconstruction.type: ["mse", "mae", "huber"]
      # feature_processing.pca.variance_threshold: [0.90, 0.95, 0.99]
    
    # Number of runs (for random/bayesian)
    n_runs: 20
    
    # Early stopping for sweeps
    early_stopping:
      metric: "val_loss"
      patience: 5
      min_delta: 0.001

# Project Settings
project:
  name: "urban-embedding"
  root_dir: "C:/Users/Bert Berkers/PycharmProjects/UrbanRepML"
  
  # Experiment tracking
  tracking:
    # Track experiment lineage
    track_git_hash: true
    track_config_changes: true
    save_all_configs: true

# Study Area Configuration
study_area:
  # Primary study area
  name: "south_holland_threshold80"
  
  # For comparative experiments
  comparison_areas: []  # ["south_holland_threshold70", "south_holland_threshold90"]
  
  # Data organization
  data_root: "${project.root_dir}/data/study_areas"
  
  # Study area specific paths
  paths:
    base: "${study_area.data_root}/${study_area.name}"
    embeddings: "${study_area.paths.base}/embeddings"
    preprocessed: "${study_area.paths.base}/preprocessed [TODO SORT & CLEAN UP]"
    raw: "${study_area.paths.base}/raw"  # For original data if needed

# Data Configuration
data:
  # Input embeddings configuration
  embeddings:
    # Each modality can have multiple variants for experiments
    modalities:
      gtfs:
        primary: "embeddings_GTFS_10.parquet"
        variants:
          # v2: "embeddings_GTFS_10_v2.parquet"  # Alternative versions
        enabled: true
        required: true
        
      roadnetwork:
        primary: "embeddings_roadnetwork_10.parquet"
        variants: {}
        enabled: true
        required: true
        
      aerial:
        primary: "embeddings_aerial_10_finetune.parquet"
        variants:
          # no_finetune: "embeddings_aerial_10.parquet"
        enabled: true
        required: true
        
      poi:
        primary: "embeddings_POI_hex2vec_10.parquet"
        variants: {}
        enabled: true
        required: false  # Can run without POI data
    
    # Embedding combination strategies for experiments
    combination_strategy: "concatenate"  # Options: concatenate, weighted_sum, attention
    
  # Preprocessed data files
  preprocessed:
    required_files:
      regions:
        - "area_study_gdf.parquet"
        - "regions_8_gdf.parquet"
        - "regions_9_gdf.parquet"
        - "regions_10_gdf.parquet"
      density:
        - "building_density_res8_preprocessed.parquet"
        - "building_density_res9_preprocessed.parquet"
        - "building_density_res10_preprocessed.parquet"
      mappings:
        - "mappings_8-9.parquet"
        - "mappings_9-10.parquet"
        - "mappings_8-10.parquet"
    
    optional_files:
      networks:
        - "osm_network_walk_speeds.parquet"
        - "osm_network_cycle_speeds.parquet"
        - "osm_network_drive_speeds.parquet"
      auxiliary:
        - "land_use.parquet"
        - "demographics.parquet"

# Model Configuration
model:
  # Always use UrbanUNet architecture
  architecture: "UrbanUNet"
  
  # Graph convolution type
  conv_type: "gcn"  # Options: gcn, gat, sage
  
  # Model parameters
  params:
    hidden_dim: 128
    output_dim: 32
    num_convs: 6
    dropout: 0.1
    activation: "relu"
    normalization: "batch"
    
    # GAT-specific parameters (used when conv_type: "gat")
    gat:
      num_heads: 4
      concat_heads: true
      edge_dim: null
      
    # GraphSAGE-specific parameters (used when conv_type: "sage")
    sage:
      aggregator: "mean"  # Options: mean, max, lstm
      normalize: true

# Feature Processing Pipeline
feature_processing:
  # PCA configuration
  pca:
    variance_threshold: 0.95
    max_components: 32
    min_components:
      aerial: 16
      gtfs: 16
      roadnetwork: 16
      poi: 16
    eps: 1e-8
    
  # Feature scaling
  scaling:
    method: "standard"  # Options: standard, minmax, robust
    
  # Feature selection (for ablation studies)
  selection:
    method: "all"  # Options: all, variance_threshold, mutual_info
    params: {}

# Graph Construction Pipeline
graph_construction:
  # Resolution-specific configurations
  resolutions:
    8:
      mode: "drive"
      params:
        speed: 11.11      # m/s
        max_time: 600     # seconds
        search_radius: 300 # meters
        beta: 0.0008      # decay parameter
        
    9:
      mode: "bike"
      params:
        speed: 4.17
        max_time: 450
        search_radius: 150
        beta: 0.0012
        
    10:
      mode: "walk"
      params:
        speed: 1.4
        max_time: 300
        search_radius: 75
        beta: 0.0020
  
  # Graph construction strategies
  edge_construction:
    method: "accessibility"  # Options: accessibility, knn, radius
    weighted: true
    directed: false
    
  # Graph augmentation options
  augmentation:
    add_self_loops: true
    normalize_edges: true

# Training Configuration
training:
  # Optimizer settings
  optimizer:
    type: "adam"  # Options: adam, adamw, sgd
    params:
      lr: 1e-5
      weight_decay: 1e-5
      betas: [0.9, 0.999]
  
  # Learning rate scheduler
  scheduler:
    type: "plateau"  # Options: plateau, cosine, step, exponential
    params:
      factor: 0.5
      patience: 50
      min_lr: 1e-7
  
  # Training schedule
  epochs: 10000
  warmup_epochs: 1000
  early_stopping:
    patience: 100
    min_delta: 1e-5
    monitor: "total_loss"
    
  # Loss configuration
  losses:
    reconstruction:
      weight: 1.0
      type: "mse"  # Options: mse, mae, huber
    consistency:
      weight: 3.0
      type: "mse"
    # Additional losses for experiments
    regularization:
      weight: 0.0
      type: "l2"
      
  # Gradient settings
  gradient_clip: 1.0
  accumulation_steps: 1
  
  # Checkpointing
  checkpoint:
    interval: 100
    save_best: true
    save_last: true
    save_optimizer: true

# Evaluation Configuration
evaluation:
  # Metrics to compute
  metrics:
    - "reconstruction_error"
    - "embedding_quality"
    - "cluster_validity"
    - "spatial_autocorrelation"
    
  # Validation split
  validation:
    method: "spatial"  # Options: random, spatial, temporal
    ratio: 0.2
    
  # Downstream tasks for evaluation
  downstream_tasks:
    - "urban_classification"
    - "density_prediction"

# Visualization Configuration
visualization:
  # Clustering visualization
  clustering:
    n_clusters:
      8: 8
      9: 8
      10: 8
    algorithm: "kmeans"  # Options: kmeans, hierarchical, dbscan
    
  # Plot settings
  plots:
    cmap: "Accent"
    dpi: 600
    figsize: [12, 12]
    save_formats: ["png", "pdf", "svg"]
    
  # Additional visualizations
  generate:
    cluster_maps: true
    embedding_projections: true
    loss_curves: true
    feature_importance: true
    graph_statistics: true

# Output Configuration
output:
  # Base output directory
  base_dir: "${project.root_dir}/experiments"
  
  # Experiment-specific directory structure
  experiment_dir: "${output.base_dir}/${experiment.name}/${study_area.name}/${timestamp}"
  
  # Subdirectories
  structure:
    embeddings: "${output.experiment_dir}/embeddings"
    models: "${output.experiment_dir}/models"
    plots: "${output.experiment_dir}/plots"
    logs: "${output.experiment_dir}/logs"
    configs: "${output.experiment_dir}/configs"
    metrics: "${output.experiment_dir}/metrics"
    comparisons: "${output.experiment_dir}/comparisons"  # For cross-study area comparisons
    
  # File naming conventions
  naming:
    timestamp_format: "%Y%m%d_%H%M%S"
    include_timestamp: true
    include_git_hash: true
    include_conv_type: true  # Add conv type to filename
    
  # What to save
  save:
    # Embeddings
    final_embeddings: true
    embedding_format: "parquet"  # Options: parquet, pt, npy
    
    # Models
    all_checkpoints: false
    best_checkpoint: true
    final_checkpoint: true
    checkpoint_format: "pt"
    
    # Training artifacts
    training_history: true
    loss_curves: true
    metric_history: true
    
    # Experiment metadata
    config_snapshot: true
    git_diff: true
    environment_info: true
    
  # Results aggregation for sweeps
  aggregation:
    summary_metrics: true
    best_params: true
    comparison_plots: true

# Caching Configuration
cache:
  enabled: true
  base_dir: "${project.root_dir}/cache"
  
  # Cache organization
  structure:
    study_area: "${cache.base_dir}/${study_area.name}"
    graphs: "${cache.structure.study_area}/graphs/${cache.version}"
    networks: "${cache.structure.study_area}/networks"
    pca_models: "${cache.structure.study_area}/pca_models"
    features: "${cache.structure.study_area}/features"
    
  # Cache versioning
  version: "v1"
  
  # Cache management
  max_size_gb: 50
  ttl_days: 30
  compression: "gzip"

# Logging Configuration
logging:
  # Log levels by component
  levels:
    root: "INFO"
    training: "DEBUG"
    data: "INFO"
    model: "INFO"
    
  # Where to log
  handlers:
    file:
      enabled: true
      path: "${output.structure.logs}/run.log"
      format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    console:
      enabled: true
      format: "%(levelname)s - %(message)s"
    
  # What to log
  log_every_n_epochs: 10
  log_gradients: false
  log_weights: false
  log_activations: false

# Experiment Tracking
tracking:
  # Weights & Biases
  wandb:
    enabled: true
    project: "urban-embedding-experiments"
    entity: null
    group: "${experiment.name}"
    job_type: "${experiment.mode}"
    tags: 
      - "${study_area.name}"
      - "${models.primary}"
      - "v${experiment.version}"
    
    # What to track
    log:
      metrics: true
      models: true
      code: true
      gradients: false
      system_metrics: true
      
  # MLflow (alternative)
  mlflow:
    enabled: false
    tracking_uri: "file://${output.base_dir}/mlruns"
    experiment_name: "${experiment.name}"
    
  # TensorBoard
  tensorboard:
    enabled: false
    log_dir: "${output.structure.logs}/tensorboard"

# Compute Configuration
compute:
  # Device settings
  device:
    type: "auto"  # auto, cuda, cpu, mps
    gpu_id: 0
    
  # Memory management
  memory:
    gpu_memory_fraction: 0.9
    gradient_checkpointing: false
    
  # Parallelization
  parallel:
    data_parallel: false
    model_parallel: false
    
  # Performance
  performance:
    torch_compile: false
    mixed_precision: false
    cudnn_benchmark: true

# Reproducibility
reproducibility:
  # Random seeds
  seeds:
    python: 42
    numpy: 42
    torch: 42
    
  # Deterministic behavior
  deterministic:
    cudnn: true
    operations: true
    
  # Version tracking
  track_versions:
    python: true
    packages: true
    cuda: true
    
# Advanced Features
advanced:
  # Ablation Studies
  ablation:
    enabled: false
    # Disable specific modalities to test their contribution
    disable_modalities: []  # e.g., ["poi", "gtfs"] to test without these
    # Disable specific loss components
    disable_losses: []  # e.g., ["consistency"] to test without consistency loss
    # Test different feature combinations
    feature_combinations:
      - ["aerial", "roadnetwork"]  # Test with only these features
      - ["gtfs", "poi"]
      - ["aerial", "gtfs", "roadnetwork"]  # Without POI
    
  # Comparative Analysis Across Study Areas
  comparative:
    enabled: false
    # Compare multiple study areas in one run
    study_areas_to_compare:
      - "south_holland_threshold70"
      - "south_holland_threshold80" 
      - "south_holland_threshold90"
    # Metrics to compare
    comparison_metrics:
      - "embedding_similarity"
      - "cluster_stability"
      - "spatial_autocorrelation"
    # Generate comparison reports
    generate_report: true
    
  # Loss Function Experiments
  loss_experiments:
    # Test different loss combinations
    configurations:
      baseline:
        reconstruction: 1.0
        consistency: 3.0
      reconstruction_only:
        reconstruction: 1.0
        consistency: 0.0
      balanced:
        reconstruction: 1.0
        consistency: 1.0
      consistency_heavy:
        reconstruction: 1.0
        consistency: 5.0
        
  # Optimizer Experiments  
  optimizer_experiments:
    # Test different optimizer configurations
    configurations:
      adam_default:
        type: "adam"
        lr: 1e-5
        weight_decay: 1e-5
      adamw_high_wd:
        type: "adamw"
        lr: 1e-5
        weight_decay: 1e-3
      sgd_momentum:
        type: "sgd"
        lr: 1e-4
        momentum: 0.9
        weight_decay: 1e-5

# Debug Configuration
debug:
  enabled: false
  
  # Debug options
  options:
    small_subset: false
    subset_size: 100
    save_intermediate: false
    profile_memory: false
    profile_time: false
    check_gradients: false
    
  # Visualization during debug
  visualize:
    graphs: false
    embeddings: false
    gradients: false